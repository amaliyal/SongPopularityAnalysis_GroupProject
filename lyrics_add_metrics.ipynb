{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "449eb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import median\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "60f4ea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alisayanovski/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "5374c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_ENG = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "844ddda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Takes text (str) as parameter. \n",
    "    Returns list of words from text without capital letters and punctuation.\n",
    "    '''\n",
    "    return list(filter(None, re.sub(r'[^\\w\\\\s]', ' ', text.lower()).split(' ')))\n",
    "\n",
    "def remove_stopwords(words_list, stopwords_list):\n",
    "    '''\n",
    "    Takes list of words and list of stopwords as parameters.\n",
    "    Returns list of words cleaned of stopwords.\n",
    "    '''\n",
    "    unique_words = set(words_list)\n",
    "    return [word for word in unique_words if word not in stopwords_list]\n",
    "\n",
    "def get_unique_words_list(text, stopwords_list):\n",
    "    '''\n",
    "    Takes text (str) and list of stopwords as parameters. \n",
    "    Returns list of words with no repetitions.\n",
    "    '''\n",
    "    words_list = clean_text(text)\n",
    "    unique_words = set(words_list)\n",
    "    unique_words_no_stopwords = remove_stopwords(words_list, stopwords_list)\n",
    "    \n",
    "    return unique_words_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "055b38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words_rate(text, stopwords_list):\n",
    "    '''\n",
    "    Takes text (str) and list of stopwords as parameters.\n",
    "    Returns ratio of unique words in text (float).\n",
    "    '''\n",
    "    total_words = clean_text(text)\n",
    "    unique_words_no_stopwords = get_unique_words_list(' '.join(total_words), stopwords_list)\n",
    "    \n",
    "    return round(len(unique_words_no_stopwords) / len(total_words), 2)\n",
    "\n",
    "\n",
    "def count_total_char(text):\n",
    "    '''\n",
    "    Takes text (str) as parameter. \n",
    "    Returns total number of characters without punctuation.\n",
    "    '''\n",
    "    return len(re.sub(r'[^\\w\\\\s]', '', text))\n",
    "\n",
    "\n",
    "def get_repetitions_rate(text, stopwords_list):\n",
    "    '''\n",
    "    Takes text (str) and list of stopwords as parameters. \n",
    "    Returns mean average number of repetitions of word, if it appears more than once (float).\n",
    "    '''\n",
    "    unique_words = get_unique_words_list(text, stopwords_list)\n",
    "\n",
    "    dict_words = {}\n",
    "\n",
    "    for word in text:\n",
    "        if word:\n",
    "            if word in dict_words:\n",
    "                dict_words[word] += 1\n",
    "\n",
    "            else:\n",
    "                dict_words[word] = 1\n",
    "\n",
    "    repeating_words = []  \n",
    "\n",
    "    for key, value in dict_words.items():\n",
    "        if value > 1:\n",
    "            repeating_words.append(value)\n",
    "    \n",
    "    if repeating_words:\n",
    "        return round(sum(repeating_words) / len(repeating_words), 2)\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_repetitions_rate_median(text, stopwords_list):\n",
    "    '''\n",
    "    Takes text (str) and list of stopwords as parameters. \n",
    "    Returns median average number of repetitions of word, if it appears more than once (float).\n",
    "    '''\n",
    "    unique_words = get_unique_words_list(text, stopwords_list)\n",
    "\n",
    "    dict_words = {}\n",
    "\n",
    "    for word in text:\n",
    "        if word:\n",
    "            if word in dict_words:\n",
    "                dict_words[word] += 1\n",
    "\n",
    "            else:\n",
    "                dict_words[word] = 1\n",
    "\n",
    "    repeating_words = []  \n",
    "\n",
    "    for key, value in dict_words.items():\n",
    "        if value > 1:\n",
    "            repeating_words.append(value)\n",
    "    \n",
    "    if repeating_words:\n",
    "        return median(repeating_words)\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def count_total_words(text):\n",
    "    '''\n",
    "    Takes text (str) as parameter. \n",
    "    Returns number of words (int).\n",
    "    '''\n",
    "    return len(clean_text(text))\n",
    "\n",
    "\n",
    "def get_mean_word_len(text, stopwords_list):\n",
    "    '''\n",
    "    Takes text (str) and list of stopwords as parameters.  \n",
    "    Returns average number of characters per word (float).\n",
    "    '''\n",
    "    words_list = clean_text(text)\n",
    "    total_words_no_stopwords = remove_stopwords(words_list, stopwords_list)\n",
    "\n",
    "    return round(len(''.join(total_words_no_stopwords)) / len(get_unique_words_list(text, stopwords_list)), 2)\n",
    "\n",
    "\n",
    "def get_mean_sentence_len(text):\n",
    "    '''\n",
    "    Takes text (str) as parameter.  \n",
    "    Returns average number of words per line (float).\n",
    "    '''\n",
    "    sentence_list = list(filter(None, text.split('\\n')))\n",
    "    line_len_list = [len(el) for el in sentence_list]\n",
    "    return round(sum(line_len_list) / len(sentence_list), 2)\n",
    "\n",
    "def count_word(word, text):\n",
    "    '''\n",
    "    Takes text (str) and a word as parameters.  \n",
    "    Returns number of word appearance in the text.\n",
    "    '''\n",
    "    words_list = clean_text(text)\n",
    "    return words_list.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "fc44c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(text):\n",
    "    '''\n",
    "    Takes text (str) as parameter. \n",
    "    Returns tuple of metrics for this text.\n",
    "    '''\n",
    "    STOPWORDS_LIST = stopwords.words('english')\n",
    "    \n",
    "    repetitions_rate = get_repetitions_rate(text, STOPWORDS_LIST)\n",
    "    mean_sentence_len = get_mean_sentence_len(text)\n",
    "    mean_word_len = get_mean_word_len(text, STOPWORDS_LIST)\n",
    "    unique_words_rate = count_unique_words_rate(text, STOPWORDS_LIST)\n",
    "    total_words = count_total_words(text)\n",
    "    total_char = count_total_char(text)\n",
    "\n",
    "    # new functions\n",
    "    repetitions_rate_median = get_repetitions_rate_median(text, STOPWORDS_LIST)\n",
    "    \n",
    "    # count words\n",
    "    words_counts = []\n",
    "    for word in top_repeaded_words_no_stopwords: ###\n",
    "        words_counts.append(count_word(word, text))\n",
    "        \n",
    "        \n",
    "    return (total_char, total_words, mean_word_len, mean_sentence_len, unique_words_rate, repetitions_rate,\n",
    "           repetitions_rate_median, *words_counts)\n",
    "\n",
    "\n",
    "def write_row_to_csv(df, values_tuple):\n",
    "    '''\n",
    "    Takes df and metrics tuple as parameters. \n",
    "    Writes values to a df. Df should already exist!\n",
    "    '''\n",
    "    for i in range(len(values_tuple)):\n",
    "        df['total_char'][i] = values_tuple[i][0]\n",
    "        df['total_words'][i] = values_tuple[i][1]\n",
    "        df['mean_word_len'][i] = values_tuple[i][2]\n",
    "        df['mean_sentence_len'][i] = values_tuple[i][3]\n",
    "        df['unique_words_rate'][i] =values_tuple[i][4]\n",
    "        df['repetitions_rate'][i] = values_tuple[i][5]\n",
    "        \n",
    "        #new\n",
    "        df['repetitions_rate_median'][i] = values_tuple[i][6]\n",
    "        \n",
    "        # count words\n",
    "        j = 0\n",
    "        for word in top_repeaded_words_no_stopwords: ###\n",
    "            j += 1\n",
    "            df[f'count_word_{word}'][i] = values_tuple[i][6 + j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "6f49e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_top_repeating_words(text, words_num):\n",
    "    cleaned_text = clean_text(text.replace('\\\\n', ' ').replace('\\\\', ' '))\n",
    "    \n",
    "    dict_all_words = {}\n",
    "\n",
    "    for word in cleaned_text:\n",
    "        if word:\n",
    "            if word in dict_all_words:\n",
    "                dict_all_words[word] += 1\n",
    "\n",
    "            else:\n",
    "                dict_all_words[word] = 1\n",
    "                \n",
    "    sorted_words = [k for k, v in sorted(dict_all_words.items(), key=lambda item: item[1], reverse=True)]\n",
    "    \n",
    "    return sorted_words[:words_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "b5f540b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_repeated_words = count_top_repeating_words(df_lyrics['lyrics'].to_string(index=False), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "4ed88253",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_repeaded_words_no_stopwords = [word for word in top_repeated_words if word not in STOPWORDS_ENG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "49b39707",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_raw = df_lyrics['lyrics'].to_string(index=False)\n",
    "cleaned_lyrics_list = clean_text(lyrics_raw)\n",
    "lyrics_no_stopwords = [word for word in cleaned_lyrics_list if word not in STOPWORDS_ENG]\n",
    "clean_lyrics_string = ' '.join(lyrics_no_stopwords)\n",
    "\n",
    "top_repeaded_words_no_stopwords = count_top_repeating_words(clean_lyrics_string, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "bf7832ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics = pd.read_csv('final_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "a644cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_columns = [f'count_word_{word}' for word in top_repeaded_words_no_stopwords]\n",
    "\n",
    "df_metrics = pd.DataFrame(df_lyrics, columns=['repetitions_rate',\n",
    "                                            'mean_sentence_len',\n",
    "                                            'mean_word_len',\n",
    "                                            'unique_words_rate',\n",
    "                                            'total_words',\n",
    "                                            'total_char',\n",
    "                                            'repetitions_rate_median',\n",
    "                                             *additional_columns])\n",
    "\n",
    "lyrics = df_lyrics['lyrics']\n",
    "write_row_to_csv(df_metrics, lyrics.apply(get_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "c7491418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_lyrics, df_metrics], axis=\"columns\")\n",
    "df_final.to_csv('lyrics_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "38961e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>repetitions_rate</th>\n",
       "      <th>mean_sentence_len</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>unique_words_rate</th>\n",
       "      <th>total_words</th>\n",
       "      <th>...</th>\n",
       "      <th>count_word_one</th>\n",
       "      <th>count_word_love</th>\n",
       "      <th>count_word_the</th>\n",
       "      <th>count_word_go</th>\n",
       "      <th>count_word_time</th>\n",
       "      <th>count_word_get</th>\n",
       "      <th>count_word_me</th>\n",
       "      <th>count_word_when</th>\n",
       "      <th>count_word_come</th>\n",
       "      <th>count_word_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(You Drive Me) Crazy</td>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>Crazy\\nOh, oh\\n\\nBaby, I'm so into you\\nYou go...</td>\n",
       "      <td>26dSoYclwsYLMAKD3tpOr4</td>\n",
       "      <td>1DSJNBNhGZCigg9ll5VeZv</td>\n",
       "      <td>30.41</td>\n",
       "      <td>31.68</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.15</td>\n",
       "      <td>271.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100 Years</td>\n",
       "      <td>Five For Fighting</td>\n",
       "      <td>I'm 15 for a moment\\nCaught in between 10 and ...</td>\n",
       "      <td>7FgMLbnZVrEnir95O0YujA</td>\n",
       "      <td>2lFlveK1y13WWp3vnQtrr3</td>\n",
       "      <td>29.66</td>\n",
       "      <td>26.55</td>\n",
       "      <td>4.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11 Blocks</td>\n",
       "      <td>Wrabel</td>\n",
       "      <td>11 blocks from my door to your doorstep\\nThree...</td>\n",
       "      <td>7r2uG6BlFXKcwmh9ItqlII</td>\n",
       "      <td>7nZBRPj89rgeZ5eBLp2J7P</td>\n",
       "      <td>38.60</td>\n",
       "      <td>33.20</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.19</td>\n",
       "      <td>345.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985</td>\n",
       "      <td>Bowling For Soup</td>\n",
       "      <td>Debbie just hit the wall, she never had it all...</td>\n",
       "      <td>5ND0mGcL9SKSjWIjPd0xIb</td>\n",
       "      <td>5oQcOu1omDykbIPSdSQQNJ</td>\n",
       "      <td>36.77</td>\n",
       "      <td>48.19</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.31</td>\n",
       "      <td>343.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>Anne-Marie</td>\n",
       "      <td>I will always remember\\nThe day you kissed my ...</td>\n",
       "      <td>1zNqDE7qDGCsyzJwohVaoX</td>\n",
       "      <td>2BgEsaKNfHUdlh97KmvFyo</td>\n",
       "      <td>44.02</td>\n",
       "      <td>28.54</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.19</td>\n",
       "      <td>387.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>Burnin' Up</td>\n",
       "      <td>Jonas Brothers</td>\n",
       "      <td>\\n\\nI'm hot\\nYou're cold\\nYou go around\\nLike ...</td>\n",
       "      <td>7gOdHgIoIKoe4i9Tta6qdD</td>\n",
       "      <td>2VEsmoek0sol9MnJFyoG9e</td>\n",
       "      <td>39.89</td>\n",
       "      <td>25.98</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.22</td>\n",
       "      <td>314.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>We R Who We R</td>\n",
       "      <td>Kesha</td>\n",
       "      <td>\\n\\nHot and dangerous\\nIf you're one of us, th...</td>\n",
       "      <td>6LqNN22kT3074XbTVUrhzX</td>\n",
       "      <td>3LUWWox8YYykohBbHUrrxd</td>\n",
       "      <td>47.88</td>\n",
       "      <td>38.29</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>456.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>Domino</td>\n",
       "      <td>Jessie Ware</td>\n",
       "      <td>\\n\\nWhat can I say? What can I do?\\nIf it's al...</td>\n",
       "      <td>5Mq7iqCWBzofK39FBqblNc</td>\n",
       "      <td>6MAdEUilV2p9RQUqE5bMAK</td>\n",
       "      <td>36.85</td>\n",
       "      <td>33.58</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.14</td>\n",
       "      <td>293.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>Your Love Is My Drug</td>\n",
       "      <td>Kesha</td>\n",
       "      <td>\\n\\nMaybe I need some rehab\\nOr maybe just nee...</td>\n",
       "      <td>6LqNN22kT3074XbTVUrhzX</td>\n",
       "      <td>6vc2Jq2vaGu8z326kSrw92</td>\n",
       "      <td>46.77</td>\n",
       "      <td>31.14</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.20</td>\n",
       "      <td>397.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>I Surrender All</td>\n",
       "      <td>Citizens</td>\n",
       "      <td>\\n\\nAll to Jesus I surrender\\nAll to Him I fre...</td>\n",
       "      <td>3e7KVnSiZjsBkReSv0L6db</td>\n",
       "      <td>45bthi0SK4vxduIwzS1G9y</td>\n",
       "      <td>27.23</td>\n",
       "      <td>22.02</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4391 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      song        artist_name  \\\n",
       "0     (You Drive Me) Crazy     Britney Spears   \n",
       "1                100 Years  Five For Fighting   \n",
       "2                11 Blocks             Wrabel   \n",
       "3                     1985   Bowling For Soup   \n",
       "4                     2002         Anne-Marie   \n",
       "...                    ...                ...   \n",
       "4386            Burnin' Up     Jonas Brothers   \n",
       "4387         We R Who We R              Kesha   \n",
       "4388                Domino        Jessie Ware   \n",
       "4389  Your Love Is My Drug              Kesha   \n",
       "4390       I Surrender All           Citizens   \n",
       "\n",
       "                                                 lyrics  \\\n",
       "0     Crazy\\nOh, oh\\n\\nBaby, I'm so into you\\nYou go...   \n",
       "1     I'm 15 for a moment\\nCaught in between 10 and ...   \n",
       "2     11 blocks from my door to your doorstep\\nThree...   \n",
       "3     Debbie just hit the wall, she never had it all...   \n",
       "4     I will always remember\\nThe day you kissed my ...   \n",
       "...                                                 ...   \n",
       "4386  \\n\\nI'm hot\\nYou're cold\\nYou go around\\nLike ...   \n",
       "4387  \\n\\nHot and dangerous\\nIf you're one of us, th...   \n",
       "4388  \\n\\nWhat can I say? What can I do?\\nIf it's al...   \n",
       "4389  \\n\\nMaybe I need some rehab\\nOr maybe just nee...   \n",
       "4390  \\n\\nAll to Jesus I surrender\\nAll to Him I fre...   \n",
       "\n",
       "                   artist_id                track_id  repetitions_rate  \\\n",
       "0     26dSoYclwsYLMAKD3tpOr4  1DSJNBNhGZCigg9ll5VeZv             30.41   \n",
       "1     7FgMLbnZVrEnir95O0YujA  2lFlveK1y13WWp3vnQtrr3             29.66   \n",
       "2     7r2uG6BlFXKcwmh9ItqlII  7nZBRPj89rgeZ5eBLp2J7P             38.60   \n",
       "3     5ND0mGcL9SKSjWIjPd0xIb  5oQcOu1omDykbIPSdSQQNJ             36.77   \n",
       "4     1zNqDE7qDGCsyzJwohVaoX  2BgEsaKNfHUdlh97KmvFyo             44.02   \n",
       "...                      ...                     ...               ...   \n",
       "4386  7gOdHgIoIKoe4i9Tta6qdD  2VEsmoek0sol9MnJFyoG9e             39.89   \n",
       "4387  6LqNN22kT3074XbTVUrhzX  3LUWWox8YYykohBbHUrrxd             47.88   \n",
       "4388  5Mq7iqCWBzofK39FBqblNc  6MAdEUilV2p9RQUqE5bMAK             36.85   \n",
       "4389  6LqNN22kT3074XbTVUrhzX  6vc2Jq2vaGu8z326kSrw92             46.77   \n",
       "4390  3e7KVnSiZjsBkReSv0L6db  45bthi0SK4vxduIwzS1G9y             27.23   \n",
       "\n",
       "      mean_sentence_len  mean_word_len  unique_words_rate  total_words  ...  \\\n",
       "0                 31.68           4.98               0.15        271.0  ...   \n",
       "1                 26.55           4.43               0.23        281.0  ...   \n",
       "2                 33.20           4.95               0.19        345.0  ...   \n",
       "3                 48.19           4.94               0.31        343.0  ...   \n",
       "4                 28.54           4.95               0.19        387.0  ...   \n",
       "...                 ...            ...                ...          ...  ...   \n",
       "4386              25.98           4.64               0.22        314.0  ...   \n",
       "4387              38.29           5.01               0.16        456.0  ...   \n",
       "4388              33.58           5.30               0.14        293.0  ...   \n",
       "4389              31.14           5.13               0.20        397.0  ...   \n",
       "4390              22.02           5.18               0.18        188.0  ...   \n",
       "\n",
       "      count_word_one  count_word_love  count_word_the  count_word_go  \\\n",
       "0                1.0              0.0             4.0            0.0   \n",
       "1                0.0              0.0             7.0            0.0   \n",
       "2                0.0              2.0             7.0            0.0   \n",
       "3                2.0              0.0             9.0            0.0   \n",
       "4                3.0             10.0            26.0            3.0   \n",
       "...              ...              ...             ...            ...   \n",
       "4386             0.0              0.0            15.0            1.0   \n",
       "4387             1.0              1.0             7.0            4.0   \n",
       "4388             0.0              0.0             1.0            0.0   \n",
       "4389             0.0             55.0             5.0            0.0   \n",
       "4390             0.0              2.0             4.0            0.0   \n",
       "\n",
       "      count_word_time  count_word_get  count_word_me  count_word_when  \\\n",
       "0                 1.0             0.0           17.0              0.0   \n",
       "1                11.0             0.0            0.0              3.0   \n",
       "2                 0.0             2.0            4.0              4.0   \n",
       "3                 1.0             1.0            0.0              4.0   \n",
       "4                 3.0             0.0            9.0              4.0   \n",
       "...               ...             ...            ...              ...   \n",
       "4386              0.0             1.0            3.0              0.0   \n",
       "4387              1.0             0.0            0.0              1.0   \n",
       "4388              0.0             0.0           11.0              1.0   \n",
       "4389              2.0             3.0            2.0              2.0   \n",
       "4390              0.0             0.0            4.0              0.0   \n",
       "\n",
       "      count_word_come  count_word_all  \n",
       "0                 0.0             5.0  \n",
       "1                 0.0             1.0  \n",
       "2                 0.0             0.0  \n",
       "3                 0.0             2.0  \n",
       "4                 0.0             4.0  \n",
       "...               ...             ...  \n",
       "4386              3.0             2.0  \n",
       "4387              0.0             2.0  \n",
       "4388              0.0             2.0  \n",
       "4389              0.0             5.0  \n",
       "4390              0.0            22.0  \n",
       "\n",
       "[4391 rows x 32 columns]"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('lyrics_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f18799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
